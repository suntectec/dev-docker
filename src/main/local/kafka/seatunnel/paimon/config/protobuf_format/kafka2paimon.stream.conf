env {
  parallelism = 1
  job.mode = "STREAMING"
  checkpoint.interval = 10000
}

source {
  Kafka {
    topic = "order_protobuf_format"
    format = protobuf
    protobuf_message_name = Order
    protobuf_schema = """
      syntax = "proto3";

      package org.apache.seatunnel.format.protobuf;

      option java_outer_classname = "ProtobufE2E";

      message Order {
        int64 id = 1;
        string order_id = 2;
        int32 supplier_id = 3;
        int32 item_id = 4;
        string status = 5;
        int32 qty = 6;
        int32 net_price = 7;
        string issued_at = 8;
        string completed_at = 9;
        string spec = 10;
        string created_at = 11;
        string updated_at = 12;
      }
    """
    schema = {
      fields {
        id = "int"
        order_id = "string"
        supplier_id = "int"
        item_id = "int"
        status = "string"
        qty = "int"
        net_price = "int"
        issued_at = "string"
        completed_at = "string"
        spec = "string"
        created_at = "string"
        updated_at = "string"
        }
    }
    bootstrap.servers = "192.168.138.15:9092"
    start_mode = "earliest"
    kafka.config = {
      client.id = "client_1"
      group.id = "poc-group"
    }
  }
}

sink {
  Paimon {
    warehouse = "s3a://tmp/kafka/seatunnel/"
    database="${database_name}_test"
    table="${table_name}_test"
    paimon.hadoop.conf = {
      fs.s3a.access-key=minioadmin
      fs.s3a.secret-key=minioadmin
      fs.s3a.endpoint="http://192.168.138.15:9000"
      fs.s3a.path.style.access=true
      fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    }
    paimon.table.write-props = {
      bucket = -1
      dynamic-bucket.target-row-num = 50000
    }
    paimon.table.primary-keys = "id"
  }
}